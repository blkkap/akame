

# Data
- First me must figure out how to get a corpus of data from wikipedia
- Use Wikipedia libary to gather plain text
- Build a fucntion that will grab pages based off of a file that it takes in with a list of pages
  - The function will first take in a list of topics
  - It will downlaod based off the list
  - It will store those pages into a folder /data/Topic/pageContent


- New way to get data from wikipedia
  - Figure out if the html is same throughout the each article within wikipedia
  - If it is we can use BS4 and pass values representing what it will scrape for
- Okay i think i found a better approach which i explained above BS4 and requests...... I HOPE it works against larger list of URLs..
  - implement a way to take in a txt like of urls
  - save to a folder with that given topic/title
  - Create new fucntion that reads from a txt file and passes each line into a function that gets that data


# NN
- Now to train a NN.. But How? 
  - GOAL of Our NN?
    - Jarvis
    - How many features, what are the labels




# DS
- List or Set
  - if the topics.txt file grows what are chances of duplictes being placed??


# Neural Network
- i have no clue wth to do here..... jk or am i?? 
