

# Data
- First me must figure out how to get a corpus of data from wikipedia
- Use Wikipedia libary to gather plain text
- Build a fucntion that will grab pages based off of a file that it takes in with a list of pages
  - The function will first take in a list of topics
  - It will downlaod based off the list
  - It will store those pages into a folder /data/Topic/pageContent


- New way to get data from wikipedia
  - Figure out if the html is same throughout the each article within wikipedia
  - If it is we can use BS4 and pass values representing what it will scrape for


# DS
- List or Set
  - if the topics.txt file grows what are chances of duplictes being placed??


# Neural Network
- i have no clue wth to do here..... jk or am i?? 
